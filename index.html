<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users">
  <meta property="og:title" content="WheelPose"/>
  <meta property="og:description" content="Project Page for WheelPose Presented at CHI 2024"/>
  <meta property="og:url" content="https://hilab-open-source.github.io/wheelpose/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="WheelPose">
  <meta name="twitter:description" content="Project Page for WheelPose Presented at CHI 2024">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="HCI, wheelchair, computer vision, data, CHI, accessibility, computer science">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users</title>
  <link rel="icon" type="image/x-icon" href="static/images/wheelchairICO.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://whuang37.github.io/" target="_blank">William Huang</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://thesamg.github.io/" target="_blank">Sam Ghahremani</a><sup></sup>,</span>
                  <span class="author-block">
					  <a href="https://www.sypei.com/" target="_blank">Siyou Pei</a><sup></sup>,</span>
					  <span class="author-block">
						  <a href="https://yangzhang.dev/" target="_blank">Yang Zhang</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of California, Los Angeles<br>CHI 2024</span>
                    <!--span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span-->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://dl.acm.org/doi/10.1145/3613904.3642555" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2404.17063" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/hilab-open-source/wheelpose" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                    
                    <!-- Video Link link -->
                    <span class="link-block">
                        <a href="https://youtu.be/KWVG5bVYwd4?si=CmDDWEzsOoyd3b-B" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-video"></i>
                        </span>
                        <span>Video</span>
                      </a>
                    </span>
					
                    <!-- UIC Data link -->
                    <span class="link-block">
                        <a href="https://drive.google.com/file/d/1ht-mbDtSf6yedbbRlvtR3b9RxzV2mKGC/view" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-images"></i>
                        </span>
                        <span>UIW Dataset</span>
                      </a>
                    </span>
                    
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/lab_30s_noaudio.mp4"
        type="video/mp4">
      </video>
      <!-- h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2 -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing pose estimation models perform poorly on wheelchair users due to a lack of representation in training data. We present a data synthesis pipeline to address this disparity in data collection and subsequently improve pose estimation performance for wheelchair users. Our configurable pipeline generates synthetic data of wheelchair users using motion capture data and motion generation outputs simulated in the Unity game engine. We validated our pipeline by conducting a human evaluation, investigating perceived realism, diversity, and an AI performance evaluation on a set of synthetic datasets from our pipeline that synthesized different backgrounds, models, and postures. We found our generated datasets were perceived as realistic by human evaluators, had more diversity than existing image datasets, and had improved person detection and pose estimation performance when fine-tuned on existing pose estimation models. Through this work, we hope to create a foothold for future efforts in tackling the inclusiveness of AI in a data-centric and human-centric manner with the data synthesis techniques demonstrated in this work. Finally, for future works to extend upon, we open source all code in this research and provide a fully configurable Unity Environment used to generate our datasets. In the case of any models we are unable to share due to redistribution and licensing policies, we provide detailed instructions on how to source and replace said models. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/example_scene.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Examples of a scene being generated. Notice the random placement of wheelchair users, different occluder objects, different lighting conditions, and various SynthHome backgrounds. The red, green, and blue arrows represent the camera coordinates which we used to insert randomization in forms of Cartesian translations and Euler rotations.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/example_synthetic_data.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          A set of example final RGB images from the WheelPose data generation pipeline. The top row is unmodified output, while the bottom row is the same set of images with their 2D bounding box and 3D keypoint locations superimposed on top.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/example_synthetic_data_1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         An additional set of 3 example RBG output images.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/motionsequence.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Example pose frames in their motion sequences resulting from our pose frame generation. Each column is an individual animation with pose frames selected in chronological order from back to front
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Methodology -->
<section class="hero is-small is-light">
  <div class="hero-body">
  <div class="container">
  <h2 class="title is-3">Methodology Pipeline</h2>
   <div class="item">
	<!-- Your image here -->
	<img src="static/images/data_pipeline.png" alt="methodology pipeline"/>
	<h2 class="subtitle is-centered has-text-centered">
	  A condensed pipeline of our data generation process. In our first step under Initial Motion Generation, the user can choose the source of their human animations. The inputed animation data is then converted into a workable 3D motion sequence format so it can be modified to fit a user in a wheelchair. These motion sequences are then converted into actual animation files to be placed in our Unity Simulation Environment. The user can now choose what kind of background images and a variety of other simulation parameters to generate machine learning training data.
	</h2>
  </div> 
</div>  
</div>
</div>
</section>


<!-- Youtube video -->
<section class="hero is-small ">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
			<iframe src="https://www.youtube.com/embed/KWVG5bVYwd4?si=Ir8EHlCLm4Xq2vlB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            <!-- iframe src="https://www.youtube.com/embed/JkaxUblCGz0aMljnApWSoo?si=WFxxms4HhICyfxkC" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
	<h2 class="title is-3">Detection Improvements (Red = Base ImageNet, Green = After WheelPose)</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
		<div style="text-align: center;">
        <img src="static/images/6_overlayed_box_s_1000.png" alt="MY ALT TEXT"/>
		</div>
        <h2 class="subtitle has-text-centered">
          Red is the base detectron2 model and Green is the same model fine tuned with out WheelPose data (true for the following images). A clear improvment is shown with the bounding box detection of the entire wheelchair, rather than just the upper body.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
		<div style="text-align: center;">
        <img src="static/images/z10_overlayed_box_s_1000.png" alt="MY ALT TEXT"/>
		</div>
        <h2 class="subtitle has-text-centered">
          Another bounding box improvement is shown with the entire wheelchair capture in a rear-facing and extreme lighting scenario.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
		<div style="text-align: center;">
        <img src="static/images/z12_overlayed_box.png" alt="MY ALT TEXT"/>
		</div>
        <h2 class="subtitle has-text-centered">
		  Here, we are able to detect the main subject of the image which was previously ignored by detectron2.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
	  <div style="text-align: center;">
      <img src="static/images/s_1_overlayed.png" alt="MY ALT TEXT"/>
	  </div>
      <h2 class="subtitle has-text-centered">
          An example of pose detection is shown. Here we can see a much improved prediction of lowerbody keypoints in a rear-facing scenario.
      </h2>
    </div>
	<div class="item">
      <!-- Your image here -->
	  <div style="text-align: center;">
		<img src="static/images/s_3_overlayed.png" alt="MY ALT TEXT"/>
	  </div>
      <h2 class="subtitle has-text-centered">
        Due to the large variety in our generated training data, our WheelPose model is able to place keypoint predictions on the wheelchair user in an extreme lighting scenario.
      </h2>
    </div>
	<div class="item">
      <!-- Your image here -->
	  <div style="text-align: center;">
      <img src="static/images/s_7_overlayed_kp.png" alt="MY ALT TEXT"/>
	  </div>
      <h2 class="subtitle has-text-centered">
        Here, the Wheelpose model is able to accurately place ankle keypoints on the actual limbs of the wheelchair user, rather than on the body of the wheelchair itself.
      </h2> 
    </div>
  </div>
</div>
</div>
</section>
<!-- End video carousel -->



<!-- Paper poster -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Users in Wheelchairs (UIW)</h2>

      <iframe  src="static/pdfs/SURP_FinalPoster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{10.1145/3613904.3642555,
		author = {Huang, William and Ghahremani, Sam and Pei, Siyou and Zhang, Yang},
		title = {WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users},
		year = {2024},
		isbn = {9798400703300},
		publisher = {Association for Computing Machinery},
		address = {New York, NY, USA},
		url = {https://doi.org/10.1145/3613904.3642555},
		doi = {10.1145/3613904.3642555},
		booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
		articleno = {944},
		numpages = {25},
		keywords = {Accessibility, Data Synthesis, Pose Estimation, Wheelchair Users},
		location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
		series = {CHI '24}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
